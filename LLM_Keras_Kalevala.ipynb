{
 "cells": [
  {
   "cell_type": "code",
   "id": "6bfdf5fa-b72c-41e2-9f58-fe30746d2f87",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6bfdf5fa-b72c-41e2-9f58-fe30746d2f87",
    "outputId": "c72217e2-f202-4e9f-88aa-61020cc930a4"
   },
   "source": [
    "# Cell 1: Setup and imports\n",
    "import os\n",
    "os.environ['KERAS_BACKEND'] = 'tensorflow'  # You can change to 'jax' or 'torch' if preferred\n",
    "\n",
    "import numpy as np\n",
    "import keras\n",
    "import sentencepiece as spm\n",
    "# Needs: conda install -c conda-forge sentencepiece\n",
    "\n",
    "print(f\"Keras version: {keras.__version__}\")\n",
    "print(f\"Keras backend: {keras.config.backend()}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a8254ae1-9919-49af-83ee-4c37de810a3e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a8254ae1-9919-49af-83ee-4c37de810a3e",
    "outputId": "cc0bd3cf-faa1-4212-c85c-9f3aa1fffac9"
   },
   "source": [
    "# Cell 2: Load and prepare text data\n",
    "# Load the Kalevala text\n",
    "with open('./kalevala_puhdas.txt', 'r', encoding='utf-8-sig') as file:\n",
    "    text = file.read()#.lower()\n",
    "\n",
    "print(f\"Text length: {len(text)} characters\")\n",
    "print(f\"First 100 characters: {text[:100]}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "786964fb-8bb5-44ed-a94e-4b716745a2da",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "786964fb-8bb5-44ed-a94e-4b716745a2da",
    "outputId": "c5f7f2f3-3bef-4aaf-b3cd-87a7f228c785"
   },
   "source": [
    "# Cell 3: Train SentencePiece model\n",
    "# Save text to a temporary file for SentencePiece training\n",
    "temp_file = 'kalevala_temp.txt'\n",
    "with open(temp_file, 'w', encoding='utf-8') as f:\n",
    "    f.write(text)\n",
    "\n",
    "# Train SentencePiece model\n",
    "vocab_size = 40000  # You can adjust this based on your needs\n",
    "model_prefix = 'kalevala_sp'\n",
    "\n",
    "'''\n",
    "spm.SentencePieceTrainer.train(\n",
    "    input=temp_file,\n",
    "    model_prefix=model_prefix,\n",
    "    vocab_size=vocab_size,\n",
    "    character_coverage=1.0,  # Important for Finnish\n",
    "    model_type='bpe',\n",
    "    user_defined_symbols=['<PAD>', '<UNK>']\n",
    ")\n",
    "'''\n",
    "\n",
    "# Load the trained tokenizer\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load(f\"{model_prefix}.model\")\n",
    "\n",
    "# Test tokenization\n",
    "test_text = \"Vaka vanha Väinämöinen\"\n",
    "tokens = sp.encode_as_pieces(test_text)\n",
    "print(f\"Tokenized example: {tokens}\")\n",
    "print(f\"Vocabulary size: {sp.get_piece_size()}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "05519eae-a2c3-4c65-b622-30398990246d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "05519eae-a2c3-4c65-b622-30398990246d",
    "outputId": "21acd9ab-089f-425e-8cab-24bacc7f1ba0"
   },
   "source": [
    "# Cell 4: Prepare training data\n",
    "# Tokenize the text\n",
    "seq_length = 64\n",
    "pieces = sp.encode_as_ids(text)\n",
    "print(f\"Total tokens: {len(pieces)}\")\n",
    "\n",
    "# Create sequences\n",
    "sequences = []\n",
    "for i in range(0, len(pieces) - seq_length):\n",
    "    # Input: first seq_length tokens, Target: next seq_length tokens (shifted by 1)\n",
    "    sequences.append(pieces[i:i+seq_length+1])\n",
    "\n",
    "# Convert to NumPy arrays\n",
    "sequences = np.array(sequences)\n",
    "inputs = sequences[:, :-1]  # All tokens except the last one\n",
    "targets = sequences[:, 1:]  # All tokens except the first one\n",
    "\n",
    "print(f\"Number of sequences: {len(sequences)}\")\n",
    "print(f\"Input shape: {inputs.shape}\")\n",
    "print(f\"Target shape: {targets.shape}\")\n",
    "\n",
    "# Split into training and validation sets\n",
    "indices = np.arange(len(sequences))\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "train_size = int(0.8 * len(sequences))\n",
    "train_indices = indices[:train_size]\n",
    "val_indices = indices[train_size:]\n",
    "\n",
    "train_inputs, train_targets = inputs[train_indices], targets[train_indices]\n",
    "val_inputs, val_targets = inputs[val_indices], targets[val_indices]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "aa629790-afa2-4958-8f42-f886511389b6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "aa629790-afa2-4958-8f42-f886511389b6",
    "outputId": "693ab5a0-5577-4fa9-db82-c2b89d51f618"
   },
   "source": [
    "# Cell 5: Define the model\n",
    "def get_positional_encoding(max_len, d_model):\n",
    "    \"\"\"Create sinusoidal positional encoding.\"\"\"\n",
    "    positions = np.arange(max_len)[:, np.newaxis]\n",
    "    angles = np.arange(d_model)[np.newaxis, :] / d_model\n",
    "    angles = 1 / (10000**angles)\n",
    "\n",
    "    pos_encoding = positions * angles\n",
    "    pos_encoding[:, 0::2] = np.sin(pos_encoding[:, 0::2])\n",
    "    pos_encoding[:, 1::2] = np.cos(pos_encoding[:, 1::2])\n",
    "\n",
    "    return pos_encoding\n",
    "\n",
    "# Define model parameters\n",
    "embed_dim = 256\n",
    "num_heads = 4\n",
    "ff_dim = 512\n",
    "num_layers = 3\n",
    "\n",
    "# Create the model\n",
    "inputs = keras.Input(shape=(seq_length,))\n",
    "embedding_layer = keras.layers.Embedding(sp.get_piece_size(), embed_dim)(inputs)\n",
    "\n",
    "# Add positional encoding\n",
    "pos_encoding = get_positional_encoding(seq_length, embed_dim)\n",
    "x = embedding_layer + pos_encoding\n",
    "\n",
    "# Helper function to create causal attention mask\n",
    "def create_causal_mask(size):\n",
    "    \"\"\"Create a causal attention mask to prevent looking at future tokens.\"\"\"\n",
    "    mask = 1 - np.triu(np.ones((size, size)), k=1)\n",
    "    return mask  # Lower triangular matrix\n",
    "\n",
    "# Transformer blocks\n",
    "for _ in range(num_layers):\n",
    "    # Multi-head attention with causal mask\n",
    "    # Manually create causal mask since use_causal_mask parameter isn't available\n",
    "    causal_mask = create_causal_mask(seq_length)\n",
    "\n",
    "    # Apply attention with manual causal mask\n",
    "    attention_output = keras.layers.MultiHeadAttention(\n",
    "        num_heads=num_heads,\n",
    "        key_dim=embed_dim // num_heads\n",
    "    )(x, x, attention_mask=causal_mask)\n",
    "\n",
    "    # Add & Norm\n",
    "    x = keras.layers.LayerNormalization(epsilon=1e-6)(x + attention_output)\n",
    "\n",
    "    # Feed-forward network\n",
    "    ffn = keras.Sequential([\n",
    "        keras.layers.Dense(ff_dim, activation=\"relu\"),\n",
    "        keras.layers.Dense(embed_dim),\n",
    "        keras.layers.Dropout(0.1)\n",
    "    ])\n",
    "    ffn_output = ffn(x)\n",
    "\n",
    "    # Add & Norm\n",
    "    x = keras.layers.LayerNormalization(epsilon=1e-6)(x + ffn_output)\n",
    "\n",
    "# Final output layer\n",
    "outputs = keras.layers.Dense(sp.get_piece_size())(x)\n",
    "\n",
    "# Create model\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "# Compile model\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=3e-4),\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model.summary()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d23f97e8-0373-4e1e-899f-dbd1553bd43b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d23f97e8-0373-4e1e-899f-dbd1553bd43b",
    "outputId": "71e8acf8-0148-42d0-9f87-73c37aae50d3"
   },
   "source": [
    "# Cell 6: Train the model\n",
    "batch_size = 64\n",
    "epochs = 10\n",
    "\n",
    "history = model.fit(\n",
    "    train_inputs, train_targets,\n",
    "    validation_data=(val_inputs, val_targets),\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    callbacks=[\n",
    "        keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True),\n",
    "        keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=1),\n",
    "        keras.callbacks.ModelCheckpoint('kalevala_best_model.keras', save_best_only=True)\n",
    "    ]\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "cf6e897e-6fbd-4925-baaf-604b7e38ca7c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 507
    },
    "id": "cf6e897e-6fbd-4925-baaf-604b7e38ca7c",
    "outputId": "10cb0fee-b548-49f9-ac50-7aad80546c39"
   },
   "source": [
    "# Cell 7: Plot training metrics\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Model Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "aa40d9b2-d512-4f7e-9a50-3439fe335967",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aa40d9b2-d512-4f7e-9a50-3439fe335967",
    "outputId": "b4cf6568-a60e-4ca8-94b7-b145729afaf8"
   },
   "source": [
    "# Cell 8: Save the model\n",
    "model.save('kalevala_model.keras')\n",
    "print(\"Model saved as 'kalevala_model.keras'\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a998c8d3-bb56-4f56-b5a2-cac6275fd26f",
   "metadata": {
    "id": "a998c8d3-bb56-4f56-b5a2-cac6275fd26f"
   },
   "source": [
    "def generate_text(model, sp, prompt, num_tokens=100, temperature=1.0):\n",
    "    \"\"\"Generate text based on a prompt with proper lowercase handling.\"\"\"\n",
    "    # Convert prompt to lowercase to match training data\n",
    "    lowercase_prompt = prompt#.lower()\n",
    "\n",
    "    # Encode the prompt\n",
    "    input_ids = sp.encode_as_ids(lowercase_prompt)\n",
    "\n",
    "    # Rest of your generation code stays the same...\n",
    "    if len(input_ids) < seq_length:\n",
    "        padding_length = seq_length - len(input_ids)\n",
    "        input_ids = [0] * padding_length + input_ids\n",
    "    else:\n",
    "        padding_length = 0\n",
    "        input_ids = input_ids[-seq_length:]\n",
    "\n",
    "    # Generated tokens\n",
    "    generated_ids = list(input_ids[padding_length:])\n",
    "\n",
    "    # Generate text token by token\n",
    "    for _ in range(num_tokens):\n",
    "        x = np.array([input_ids])\n",
    "        predictions = model.predict(x, verbose=0)[0]\n",
    "        logits = predictions[-1]\n",
    "        logits = logits / temperature\n",
    "        exp_logits = np.exp(logits - np.max(logits))\n",
    "        probs = exp_logits / np.sum(exp_logits)\n",
    "        next_token = np.random.choice(len(probs), p=probs)\n",
    "        generated_ids.append(next_token)\n",
    "        input_ids = input_ids[1:] + [next_token]\n",
    "\n",
    "    # Decode the generated sequence\n",
    "    generated_text = sp.decode(generated_ids)\n",
    "\n",
    "    return generated_text"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d9580493-d3a5-4cfc-9943-fe99dcb8e01d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d9580493-d3a5-4cfc-9943-fe99dcb8e01d",
    "outputId": "3489ff49-3d77-4f36-f73a-65a9a47324b3"
   },
   "source": [
    "# Cell 10: Generate sample text\n",
    "prompts = [\n",
    "    \"Vaka vanha Väinämöinen\",\n",
    "    \"Mieleni minun tekevi\",\n",
    "    \"Lemminkäinen\",\n",
    "    \"Pohjan neito\"\n",
    "]\n",
    "\n",
    "for prompt in prompts:\n",
    "    print(f\"\\nPrompt: {prompt}\")\n",
    "    generated = generate_text(model, sp, prompt, num_tokens=100, temperature=1.2)\n",
    "    print(generated)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "de1a2259-373c-4fed-a690-8e96496e04fc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "id": "de1a2259-373c-4fed-a690-8e96496e04fc",
    "outputId": "aaf73c89-7abe-424d-84b8-e6d8b1ec11cc"
   },
   "source": [
    "# Cell 11: Load model (if you're starting a new session)\n",
    "# Uncomment these lines to load a previously saved model\n",
    "\"\"\"\n",
    "# Load saved model\n",
    "model = keras.models.load_model('kalevala_model.keras')\n",
    "\n",
    "# Load SentencePiece tokenizer\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load('kalevala_sp.model')\n",
    "\n",
    "# Test generation\n",
    "prompt = \"Mieleni minun tekevi\"\n",
    "generated = generate_text(model, sp, prompt, num_tokens=150, temperature=1.0)\n",
    "print(generated)\n",
    "\"\"\""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c18d278e-53a9-4f58-9c7d-17451b3946e5",
   "metadata": {
    "id": "c18d278e-53a9-4f58-9c7d-17451b3946e5"
   },
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "L4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
